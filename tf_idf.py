# -*- coding: utf-8 -*-
"""TF-IDF.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18Q-lYAYnU_wadtOdLZemubSUS516CHIe
"""

import pandas as pd

data = pd.read_csv('/content/drive/MyDrive/L4S1/FYP/my/twitter data/Kusal&Niroshan.csv', error_bad_lines=False);
data_text = data[['FULL TEXT']]

import gensim
from gensim.utils import simple_preprocess
from gensim.parsing.preprocessing import STOPWORDS
from nltk.stem import WordNetLemmatizer, SnowballStemmer
from nltk.stem.porter import *
import numpy as np
np.random.seed(2018)

import nltk
nltk.download('wordnet')
nltk.download('stopwords')

from sklearn.model_selection import train_test_split
x = data[['FULL TEXT']]

x_train, x_test = train_test_split(x,
                                   train_size = 0.67,
                                   random_state = 42)

print(f"Test labels: \n{x_test}")

x_train['index'] = x_train.index
documents = x_train

stemmer = SnowballStemmer('english')
def lemmatize_stemming(text):
    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))

def preprocess(text):
    result = []
    for token in gensim.utils.simple_preprocess(text):
        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:
            result.append(lemmatize_stemming(token))
    return result

#sinhala stop word set from UOM NLPC
nltk.corpus.stopwords.words('sinhala')

#remove stop words in each news content
#stop word text shoild be more rich
#add more words
def preprocess_sin(text):
    result=[]
    for token in text :
        if token not in nltk.corpus.stopwords.words('sinhala'):
            result.append(token)
            
    return result

#access each news content column
#sinhala
preprocessed_data = []

for columnData in documents['FULL TEXT']:
  words = []
  for word in columnData.split(' '):
    words.append(word)
  print("Original document: ")
  print(words,'\n')
  print("Stop words removed documents: ")
  td=preprocess(words)
  print(td,'\n\n')
  preprocessed_data.append(td)

processed_docs = documents['head_name'].map(preprocess_sin)

dictionary = gensim.corpora.Dictionary(preprocessed_data)

bow_corpus = [dictionary.doc2bow(doc) for doc in preprocessed_data]

tf_idf = gensim.models.TfidfModel(bow_corpus)
#for doc in tf_idf[bow_corpus]:
    #print([[dictionary[id], np.around(freq, decimals=2)] for id, freq in doc])

"""**Creating similarity measure object**"""

# building the index
sims = gensim.similarities.Similarity('/content/drive/MyDrive/L4S1/FYP/my/Sim obj/sinhala/',tf_idf[bow_corpus],
                                        num_features=len(dictionary))

x_test['index'] = x_test.index
file2_docs = x_test
query_doc = file2_docs['FULL TEXT'].map(preprocess)

#for sinhala
tweets = pd.read_csv('/content/drive/MyDrive/L4S1/FYP/my/twitter data/Kusal&Niroshan.csv')
tweets = tweets.astype(str)
tweets = tweets.drop_duplicates()
#data = data.replace(r'^\s*$',np.nan,regex=True)
#data = data.dropna()
tweets.head(3)

x_test['index'] = x_test.index
file2_docs = x_test

#access each news content column
tweets_data = []

for columnData in file2_docs['FULL TEXT']:
  words = []
  for word in columnData.split(' '):
    words.append(word)
  print("Original document: ")
  print(words,'\n')
  print("Stop words removed documents: ")
  fd=preprocess(words)
  print(fd,'\n\n')
  tweets_data.append(fd)

query_doc = tweets['FULL TEXT'].map(preprocess)

query_doc_bow = [dictionary.doc2bow(doc) for doc in tweets_data]

import numpy as np
count = 0
for doc in query_doc_bow:
  # perform a similarity query against the corpus
  query_doc_tf_idf = tf_idf[doc]
  # print(document_number, document_similarity)
  max_sim=(np.max(sims[query_doc_tf_idf]))#max from similarity values
  max_index = (np.argmax(sims[query_doc_tf_idf]))#index of max value
  print('Query Document :', tweets_data[count])
  print('Comparing Result:', max_sim) 
  print('Similar Document No.: ', max_index)
  print('\n')
  count+=1